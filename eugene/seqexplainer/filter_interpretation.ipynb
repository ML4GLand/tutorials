{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter interpretation walkthrough\n",
    "**Authorship:**\n",
    "Adam Klie (last updated: *07/12/2023*)\n",
    "***\n",
    "**Description:**\n",
    "This notebook is meant to serve as a guide for interpreting the learned filters of a trained model. It includes some background on the analysis, a worked example, and a discussion of the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Filter interpretation was one of the first methods used to probe the representations learned by sequence-based genomics models. The most commonly used approach involves identifying subsequences of a set of inputs that maximally activate a given neuron in the model. These subsequences can then be used to generate a position frequency matrix (PFM), which can be visualized as a sequence logo tht we commonly derive from experimental assays. The PFMs can also be submitted to tools like TomTom for annotation with known motif databases such as JASPAR or HOCOMOCO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a filter interpretation analysis\n",
    "Let's run a filter interpretation analysis on small model trained on human embryonic stem cell ChIP-seq data. \n",
    "\n",
    "SeqExplainer provides functions for running a step by step filter interpretation analysis from as set of sequences and a trained model to filter visualization and annotation. We will first show how to run this analysis with SeqExplainer, and then show how to use EUGENe's wrapper functions to run the same analysis with only a few commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seqexplainer package\n",
    "import os\n",
    "import seqexplainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend cloning the entire tutorials repository so that you have all the necessary intermediate files you need, but when applicable, we also provide links to download the files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to where you would like to save all your results, likely your tutorials download directory\n",
    "import os\n",
    "os.chdir(\"/cellar/users/aklie/projects/ML4GLand/tutorials\")  # TODO: change this to your own directory\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uncomment and run the following to get the model and config. You will not need to do this if you are working in your cloned tutorials repo\n",
    "#!mkdir -p $cwd/models\n",
    "#!wget https://github.com/ML4GLand/tutorials/raw/main/models/kopp21_cnn.ckpt -O $cwd/models/kopp21_cnn.ckpt\n",
    "\n",
    "#!mkdir -p $cwd/configs\n",
    "#!wget https://raw.githubusercontent.com/ML4GLand/tutorials/main/configs/kopp21_cnn.yaml -O $cwd/configs/kopp21_cnn.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the model form the checkpoint\n",
    "model_file = \"models/kopp21_cnn.ckpt\"\n",
    "model_arch = models.load_config(config_path=\"configs/kopp21_cnn.yaml\")\n",
    "model = models.SequenceModule.load_from_checkpoint(model_file, arch=model_arch.arch)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data\n",
    "We first need to load in sequences we will use to run the analysis. Any sequences could theoretically be used, but we will use the sequences from the test set of the model we loaded in above. These are available as a downloaded zarr store from SeqDatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data we will use here is available as a seqdataset\n",
    "import seqdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the test data\n",
    "sdata_test = seqdatasets.kopp21(task=\"binary\", split=\"test\")\n",
    "sdata_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current version of SeqExplainer's filter interpretation functions requires that we feed the model torch tensors, so we will also convert the sequences to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the seqs froma the seqdaata\n",
    "seqs = sdata_test[\"ohe_seq\"].transpose(\"_sequence\", \"_ohe\", \"length\").to_numpy()\n",
    "seqs_torch = torch.tensor(seqs, dtype=torch.float32).to(model.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get activations for test sequences\n",
    "We first need to get all the activations from passing the inputs through the layer or neuron of interest. Depending on model architecture, and how the model was defined  (i.e. the way the nn.Module's init function looks), we can use different code to get the activations. We will show how to do this using EUGENe's wrapper functions first, starting by seeing what layers are available to get activations from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what layers are available and what their names are\n",
    "models.list_available_layers(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that we can pull the layer outputs from the revcomp layer, the first conv layer, and the first max pool layer and so on. We typically care about the first convolutional layer, so we will grab from that one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first layer of the model\n",
    "layer_name = \"arch.conv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the seqexplainer function for getting the outputs from a layer\n",
    "from seqexplainer import get_layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the seqexplainer function for getting the outputs from a layer, see the API docs for more info\n",
    "activations_se = get_layer_outputs(\n",
    "    model=model,\n",
    "    inputs=seqs_torch,\n",
    "    layer_name=layer_name,\n",
    "    batch_size=32,\n",
    "    device=\"cuda\",\n",
    "    verbose=True\n",
    ")\n",
    "activations_se.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that we have a set of activations for each sequence, kernel and position of the input sequence (ignoring the ends because we used valid padding here). This is what we want!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Wait! There are a couple problems with what we just did. \n",
    "\n",
    "1. We are using the outputs from the first convolutional layer, but we are not using the activations from the first convolutional layer. If we look back at the layers that we can pull from, we can see that there is no ReLU available. It turns out, that with this particular architecture, the functional ReLU was used, meaning that we can't access it the way we would if it was defined as an nn.Module.\n",
    "\n",
    "2. The `get_layer_outputs` function calculates the outputs all the way through the model! With this model we are passing the inputs through a revcomp layer first (which reverse complements the sequence), and then passing both the revcomp and the initial input through the model. This means that activations we grab from the output of the first convolutional layer could be from either sequence, but we don't know which! If we naively take seqlets corresponding to these activation from the forward strand, we may be missing the true seqlets that correspond to the activations from the reverse strand. This will likely cause the PFMs to generate to be very noisy (or not just completely random)\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, SeqExplainer can handle this without having to write too much extra code. We just need to grab the layer from the model and directly pass sequences through (don't forget the activation!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also grab the activations directly by nabbing the layer from the model\n",
    "layer = models.get_layer(model, layer_name).to(\"cuda\")\n",
    "activations = F.relu(layer(seqs_torch.to(\"cuda\"))).detach().cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that we are on our way!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying maximally activating seqlets\n",
    "Next we need to find where the activations are high and pull out the subsequences (of the same size as the kernel) that maximally activate each filter. We can do this using one of two functions in SeqExplainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqexplainer import get_activators_n_seqlets, get_activators_max_seqlets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we will focus on the first one, in which we simply choose the top n activations for each filter. Below we try out a few values of n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This first one simply grabs the top n seqlets from the activations and sequences we pass in\n",
    "mas_100_seqs = get_activators_n_seqlets(\n",
    "    activations=activations,\n",
    "    sequences=seqs,\n",
    "    kernel_size=11,\n",
    "    padding=0,\n",
    "    num_seqlets=100,\n",
    "    num_filters=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparisons sake, we can also grab 1000 and 10 seqlets to see how the results change\n",
    "mas_1000_seqs = get_activators_n_seqlets(\n",
    "    activations=activations,\n",
    "    sequences=seqs,\n",
    "    kernel_size=11,\n",
    "    padding=0,\n",
    "    num_seqlets=1000,\n",
    "    num_filters=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_10_seqs = get_activators_n_seqlets(\n",
    "    activations=activations,\n",
    "    sequences=seqs,\n",
    "    kernel_size=11,\n",
    "    padding=0,\n",
    "    num_seqlets=10,\n",
    "    num_filters=None\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating position frequency matrices (PFMs)\n",
    "Now that we have the maximally activating subsequences, we can create a position frequency matrix (PFM) for each filter. We can do this using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function simply stacks the activatiors on top of each other and counts up the number of times each base is present\n",
    "from seqexplainer import get_pfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_100_pfms = get_pfms(\n",
    "    filter_activators=mas_100_seqs,\n",
    "    kernel_size=11\n",
    ")\n",
    "\n",
    "mas_1000_pfms = get_pfms(\n",
    "    filter_activators=mas_1000_seqs,\n",
    "    kernel_size=11\n",
    ")\n",
    "\n",
    "mas_10_pfms = get_pfms(\n",
    "    filter_activators=mas_10_seqs,\n",
    "    kernel_size=11\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the PFMs as a sequence logos\n",
    "We can now visualize the PFMs as sequence logos using logomaker. We can do this using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqexplainer.filters._plot import plot_filter_logos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filter_logos(\n",
    "    mtxs=mas_100_pfms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filter_logos(\n",
    "    mtxs=mas_1000_pfms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filter_logos(\n",
    "    mtxs=mas_10_pfms\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that we get noisier sequence logos when using smaller and larger numbers of seqlets. An n of 100 seems to give reasonable results, so we will use that for the rest of the analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the PFMs to meme format\n",
    "There are many ways to save the PFMs to meme format, but we will show how to use another EUGENe package, called MotifData, to do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motifdata import from_kernel\n",
    "from motifdata._transform import pfms_to_ppms\n",
    "from motifdata import write_meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pfms to ppms and to a motif set\n",
    "ppms = pfms_to_ppms(mas_100_pfms.transpose(0, 2, 1), pseudocount=0)\n",
    "motif_set = from_kernel(kernel=ppms, alphabet=\"ACGT\", bg={\"A\": 0.25, \"C\": 0.25, \"G\": 0.25, \"T\": 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the motif set to a meme file\n",
    "write_meme(motif_set=motif_set, filename=\"data/mas_100.meme\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate your filers with TomTom\n",
    "Our last step of the basic filter interpretation analysis is to annotate the PFMs with TomTom. We can do this using the following function from SeqExplainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqexplainer.filters import annotate_pfms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotating filters with motifs requires that you have TomTom installed from the MEME suite. You can install it using conda:\n",
    "\n",
    "```bash\n",
    "conda install -c bioconda meme\n",
    "```\n",
    "\n",
    "It also requires that you hve a reference database (in meme format) available. You can get several useful databases from the MEME suite website. We will use the JASPAR database here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = annotate_pfms(\n",
    "    filename=\"data/mas_100.meme\",\n",
    "    motifs_db=\"/cellar/users/aklie/data/shared/meme/motif_databases/JASPAR/JASPAR2022_CORE_vertebrates_non-redundant_v2.meme\",\n",
    "    output_dir=\"output/tutorial_output/\",\n",
    "    out_filename=\"mas_100_annotation.tsv\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding overinterpretation\n",
    "This approach is valuable for intepreting representations a model has learned in it's convolutional (or other layers), but it does have limitations:\n",
    "\n",
    "1. It has been shown that these representations learned by deep neural networks (DNNs) are influenced by various factors, including architectural constraints, activation functions, and training procedures Therefore, it is crucial to note that filter analysis should be employed when a model is explicitly designed and trained to learn interpretable motif representations or as a means for exploring the impact of architecture.\n",
    "2. This analysis also does not quantify the importance of specific features and establishing their relationship with model predictions. Extensions of this approach include filter ablation experiments, where you zero out the filter and see how the model's predictions change. \n",
    "3. Inaccuracies in motif annotation from applying TomTom to learned filters have been reported, and by design, ignore novel motifs.\n",
    "\n",
    "Put succintely, DNNs (especially CNNs) do not always learn simple PWM representations of motifs in input sequences like they have sometimes been made out to. All of these limitations should be considered when interpreting the results of a filter interpretation analysis. However, this should not discourage you from trying out this approach on your model! It is still a valuable tool for understanding the representations learned by a model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 ml4gland",
   "language": "python",
   "name": "ml4gland"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
